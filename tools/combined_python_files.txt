# main.py
# main.py
import pygame
import random
import matplotlib.pyplot as plt
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from env.simulator import Simulator
import env.constants as constants

pygame.init()

# 设置全屏窗口
screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
pygame.display.set_caption("Control Panel and Game Space")

# 获取屏幕尺寸
screen_width, screen_height = screen.get_size()
#print(screen_width)
#print(screen_height)

# 字体设置
font = pygame.font.Font(None, constants.FONT_SIZE)

# 侧边栏宽度
sidebar_width = constants.CONTROL_PANEL_WIDTH

# 按钮配置
buttons = [
    pygame.Rect(50, 100, constants.BUTTON_WIDTH, constants.BUTTON_HEIGHT),
    pygame.Rect(50, 100 + (constants.BUTTON_HEIGHT + constants.BUTTON_MARGIN), constants.BUTTON_WIDTH, constants.BUTTON_HEIGHT),
    pygame.Rect(50, 100 + 2 * (constants.BUTTON_HEIGHT + constants.BUTTON_MARGIN), constants.BUTTON_WIDTH, constants.BUTTON_HEIGHT)
]

slow_button = pygame.Rect(50, 100 + 3 * (constants.BUTTON_HEIGHT + constants.BUTTON_MARGIN), constants.BUTTON_WIDTH, constants.BUTTON_HEIGHT)

# 定义游戏状态
game_state = constants.MAIN_MENU

# 初始化模拟器
sim = Simulator(screen_width, screen_height)

# 初始化帧率
fps = constants.DEFAULT_FPS
clock = pygame.time.Clock()

# 迭代计数器
iteration_count = 0

# 初始化数据记录结构
prey_counts = []
predator_counts = []
prey_born_count = 0
predator_born_count = 0

# 创建图表
fig, ax = plt.subplots(figsize=(3, 2))  # 设置图表尺寸适应侧边栏
canvas = FigureCanvas(fig)

def reset_algorithm():
    prey_algorithms = ["PPO","PPO","PPO","PPO","PPO","PPO","DDPG","DDPG","DDPG","DDPG","DDPG","DDPG","DDPG","DDPG","DDPG"]
    pred_algorithms = ["PPO","PPO","PPO","DDPG","DDPG","DDPG"]
    all_pred_algorithms = assign_algorithms_to_agents(constants.NUM_PREDATORS,pred_algorithms)
    all_prey_algorithms = assign_algorithms_to_agents(constants.NUM_PREY,prey_algorithms)
    return all_pred_algorithms,all_prey_algorithms
def assign_algorithms_to_agents(len_agents, algorithm_names):
    """
    分配算法给每个智能体。

    参数:
    - agents: 智能体列表。
    - algorithm_names: 预定义的算法名称列表。

    返回:
    - 包含算法名称的列表，长度与agents列表相同。如果算法名称不足，则用'random'补充。
    """
    assigned_algorithms = []
    for i in range(len_agents):
        if i < len(algorithm_names):
            assigned_algorithms.append(algorithm_names[i])
        else:
            assigned_algorithms.append('random')
    return assigned_algorithms
def update_plot(prey_counts, predator_counts):
    ax.clear()
    ax.plot(prey_counts, label="Prey", color='blue')
    ax.plot(predator_counts, label="Predator", color='red')
    ax.legend(loc='upper right')
    ax.set_title("Population Over Time")
    canvas.draw()

def blit_plot():
    raw_data = canvas.buffer_rgba()
    raw_data_bytes = raw_data.tobytes()
    size = canvas.get_width_height()
    plot_surface = pygame.image.fromstring(raw_data_bytes, size, "RGBA")
    return plot_surface
all_pred_algorithms,all_prey_algorithms = reset_algorithm()
# 游戏主循环
running = True
selected_agent = None

while iteration_count<10000000:
    delta_time = clock.get_time() / 1000.0  # 计算帧时间，单位为秒
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False
        elif event.type == pygame.MOUSEBUTTONDOWN:
            if game_state == constants.MAIN_MENU:
                for button in buttons:
                    if button.collidepoint(event.pos):
                        if buttons.index(button) == 0:
                            game_state = constants.IN_GAME
                            sim.initialize(all_pred_algorithms,all_prey_algorithms)
                            print("Game Started")
            elif game_state == constants.IN_GAME:
                if slow_button.collidepoint(event.pos):
                    fps = constants.SLOW_FPS if fps == constants.DEFAULT_FPS else constants.DEFAULT_FPS
                else:
                    selected_agent = sim.get_agent_info(event.pos)
                    if selected_agent is not None:
                        print(selected_agent.name)
                    else:
                        print("No agent selected.")
                    if selected_agent:
                        selected_agent.selected = True
                        for agent in sim.predators + sim.preys:
                            if agent != selected_agent:
                                agent.selected = False
                    else:
                        for agent in sim.predators + sim.preys:
                            agent.selected = False

        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_ESCAPE:
                if game_state == constants.IN_GAME:
                    game_state = constants.MAIN_MENU
                    selected_agent = None
                    for agent in sim.predators + sim.preys:
                        agent.selected = False

    screen.fill((255,255, 255))

    # 绘制侧边栏背景
    pygame.draw.rect(screen, (50, 50, 50), (0, 0, sidebar_width, screen_height))

    if game_state == constants.MAIN_MENU:
        for button in buttons:
            color = constants.BUTTON_HOVER_COLOR if button.collidepoint(pygame.mouse.get_pos()) else constants.BUTTON_COLOR
            pygame.draw.rect(screen, color, button)
            text = font.render(constants.BUTTON_TEXTS[buttons.index(button)], True, (255, 255, 255))
            screen.blit(text, (button.x + (constants.BUTTON_WIDTH - text.get_width()) // 2, button.y + (constants.BUTTON_HEIGHT - text.get_height()) // 2))

    elif game_state == constants.IN_GAME:
        sim.check_events()

        
        sim.move_models()
        sim.add_food()  # 传递时间间隔
        sim.prey_hunt()
        sim.check_collisions()
        # sim.predator_hunt()
        # new_prey_born, new_predator_born = sim.applyGeneticAlgorithm()
        sim.decrease_health()  # 更新健康值
        sim.remove_dead()  # 清理死亡个体
        iteration_count += 1  # 增加迭代计数器
        sim.draw_models(screen)

        # 每100个回合输出日志
        # if iteration_count % 10 == 0:
        #     new_prey_born, new_predator_born = sim.applyGeneticAlgorithm()
            # prey_born_count += new_prey_born
            # predator_born_count += new_predator_born
            # print(f"Iteration {iteration_count}: Current Predators: {len(sim.predators)}, New Predators Born: {predator_born_count}, Predators Died: {sim.dead_predator_count}")
            # prey_born_count = 0
            # predator_born_count = 0
            # sim.dead_predator_count = 0  # 重置死亡捕食者计数
        # if iteration_count %100 ==0:
        #     print(sim.preys[1].name,end="    ")
        #     print(sim.preys[1].health,end="    ")
        # 更新数据记录结构
        prey_counts.append(len(sim.preys))
        predator_counts.append(len(sim.predators))

        # 更新并绘制图表
        update_plot(prey_counts, predator_counts)
        plot_surface = blit_plot()
        plot_rect = plot_surface.get_rect(center=(sidebar_width // 2, screen_height // 2))
        screen.blit(plot_surface, plot_rect.topleft)

        color = constants.BUTTON_HOVER_COLOR if slow_button.collidepoint(pygame.mouse.get_pos()) else constants.BUTTON_COLOR
        pygame.draw.rect(screen, color, slow_button)
        text = font.render("Slow Down", True, (255, 255, 255))
        screen.blit(text, (slow_button.x + (constants.BUTTON_WIDTH - text.get_width()) // 2, slow_button.y + (constants.BUTTON_HEIGHT - text.get_height()) // 2))

        # 显示迭代次数
        iteration_text = font.render(f"Iteration: {iteration_count}", True, (255, 255, 255))
        screen.blit(iteration_text, (50, screen_height - 150))

        if selected_agent:
            agent_info = (
                f"{selected_agent.__class__.__name__}: "
                f"Position ({selected_agent.rect.x}, {selected_agent.rect.y}), "
                f"Velocity ({selected_agent.velocity[0]}, {selected_agent.velocity[1]}), "
                f"Health ({selected_agent.health})"
            )
            info_surface = font.render(agent_info, True, (0, 0, 0))
            screen.blit(info_surface, (50, screen_height - 100))

    pygame.display.flip()
    clock.tick(fps)

pygame.quit()


# gym_env.py

import gym
from gym import spaces
import pygame
import numpy as np
import random
from env.simulator import Simulator
import env.constants as constants
from torchrl.envs.utils import check_env_specs
from gym.utils.env_checker import check_env
from gym.envs.registration import register
from tensordict import TensorDict
import torch



class LISPredatorPreyEnv(gym.Env):
    def __init__(self):
        super(LISPredatorPreyEnv, self).__init__()
        
        # 初始化模拟器
        self.simulator = Simulator(screen_width=3840, screen_height=2160)
        self.group_map = {}
        self.current_step = 0
        self.max_steps = 10_000


        
        # 初始化观察和动作空间
        self.max_range = max(600, 1000)  
        self.zero_list = [[0 for _ in range(3)] for _ in range(25)]
        self.num_entities = constants.NUM_PREDATORS + constants.NUM_PREY
        self.new_shape = (self.num_entities, 25, 3)
        self.obs_low = np.full(self.new_shape, -self.max_range, dtype=np.float32) #inf maybe not the best choice , let us decide later
        self.obs_high = np.full(self.new_shape, self.max_range, dtype=np.float32)
        self.action_shape = (self.num_entities,2)
        self.action_speed_range = max(constants.PREY_MAX_SPEED,constants.PREY_MAX_SPEED)
        self.action_low = np.full(self.action_shape, -self.action_speed_range, dtype=np.float32)
        self.action_high = np.full(self.action_shape, self.action_speed_range, dtype=np.float32)
        # obs_low = np.array([0, 0, 0] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))
        # obs_high = np.array([max_range, max_range, max_range] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))
        # self.observation_space_shape = (constants.NUM_PREDATORS+constants.NUM_PREY) * 3 * 25
        self.observation_space = spaces.Box(low=self.obs_low, high=self.obs_high,dtype=np.float32)
        self.action_space = spaces.Box(low=self.action_low, high=self.action_high, dtype=np.float32)
        self.interation = 0
        # 调用 reset 方法初始化环境
        self.reset() 

    # def reset(self):
    #     # 重置模拟器
    #     all_pred_algorithms,all_prey_algorithms = self.reset_algorithm()
    #     self.simulator.initialize(all_pred_algorithms,all_prey_algorithms)
    #     self.map_agents_to_groups(self.simulator.predators,self.simulator.preys)
    #     # 初始化环境信息（捕食者、猎物、食物和障碍物）
    #     for predator in self.simulator.predators:
    #         self._set_agent_env(predator)
    #     for prey in self.simulator.preys:
    #         self._set_agent_env(prey)
    #     # 获取所有智能体的初始观测数据
    #     observations = {}
    #     for group_name in self.group_map.keys():
    #         observations[group_name] = []
    #         for agent in getattr(self.simulator, group_name):
    #             observations[group_name].append(agent.get_observe_info())
    #     info  = {}

    #     # 返回包含所有智能体观测数据的字典
    #     return observations,info
        
    def reset(self, seed=None, **kwargs):
        # 重置模拟器
        super().reset(seed=seed, **kwargs)
        self.group_map.clear()
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)
        allalgorithms= self.reset_algorithm()
        all_pred_algorithms, all_prey_algorithms = allalgorithms[:constants.NUM_PREDATORS],allalgorithms[constants.NUM_PREDATORS:]
        self.simulator.initialize(all_pred_algorithms, all_prey_algorithms)
        self.map_agents_to_groups(self.simulator.predators, self.simulator.preys)
        # 初始化环境信息（捕食者、猎物、食物和障碍物）
        for predator in self.simulator.predators:
            self._set_agent_env(predator)
        for prey in self.simulator.preys:
            self._set_agent_env(prey)
        # 获取所有智能体的初始观测数据
        all_observations = []
        for group_name in self.group_map.keys():
            for agent in getattr(self.simulator, group_name):
                all_observations.append(agent.get_observe_info())
        
        obs = np.array(all_observations, dtype=np.float32) #np.shape(all_observations) = (2250,3)
        # print(np.shape(all_observations))
        # obs = TensorDict({
        #     'observation': torch.tensor(obs)
        # }, batch_size=[])
        info = {}
        # print("Initial Observation:", obs)
        # assert self.observation_space.contains(obs), "Initial observation is out of bounds!"

        # 返回一个数组，符合 observation_space 的定义
        return obs, info

    def map_agents_to_groups(self,simPredators,simPreys):
        self.group_map['predators'] = [predator.name for predator in simPredators]
        self.group_map['preys'] = [prey.name for prey in simPreys]

    def reset_algorithm(self):
        prey_algorithms = ["PPO","PPO","PPO","DDPG","DDPG","DDPG"]
        pred_algorithms = ["PPO","PPO","PPO","DDPG","DDPG","DDPG"]
        all_pred_algorithms = assign_algorithms_to_agents(constants.NUM_PREDATORS,pred_algorithms)
        all_prey_algorithms = assign_algorithms_to_agents(constants.NUM_PREY,prey_algorithms)
        return all_pred_algorithms+all_prey_algorithms
        # assigned_Predalgorithms = assign_algorithms_to_agents(self.simulator.predators, all_pred_algorithms)
        # assigned_Preyalgorithms = assign_algorithms_to_agents(self.simulator.preys, all_prey_algorithms)

    def _set_agent_env(self, agent):
        agent.env_predators = self.simulator.predators
        agent.env_prey = self.simulator.preys
        agent.env_food = self.simulator.foods
        agent.env_obstacles = self.simulator.obstacles

    # def step(self, actions):
    #     new_state, rewards, dones, infos = {}, {}, {}, {}
    #     statesList,rewardsList,donesList,infosList = [],[],[],[]
    #     self.current_step += 1
    #     truncated = self.current_step >= self.max_steps
    #     # self.simulator.check_events()

        
    #     # self.simulator.move_models()

    #     # 独立处理每个组的动作
    #     for group_name in self.group_map.keys():
    #         group_actions = actions[group_name]
    #         new_state[group_name], rewards[group_name], dones[group_name], infos[group_name] = self._step_group(group_name, group_actions)
    #     self.simulator.add_food()  # 传递时间间隔
    #     self.simulator.prey_hunt()
    #     self.simulator.check_collisions()

    #     # self.simulator.predator_hunt()
    #     self.simulator.decrease_health()  # 更新健康值
    #     self.simulator.remove_dead()  # 清理死亡个体
    #     # self.simulator.draw_models(screen)

    #     # sim.check_events()

        
    #     # sim.move_models()
    #     # sim.add_food()  # 传递时间间隔
    #     # sim.prey_hunt()
    #     # sim.check_collisions()
    #     # # sim.predator_hunt()
    #     # # new_prey_born, new_predator_born = sim.applyGeneticAlgorithm()
    #     # sim.decrease_health()  # 更新健康值
    #     # sim.remove_dead()  # 清理死亡个体
    #     # iteration_count += 1  # 增加迭代计数器
    #     # sim.draw_models(screen)

    #     # new_prey_born, new_predator_born = self.simulator.applyGeneticAlgorithm()

    #     return new_state, rewards, dones, truncated, infos
    def step(self, actions):
        new_state, rewards, dones, infos = [], [], [], []
        self.current_step += 1
        truncated = self.current_step >= self.max_steps
        # 获取所有键并转换为列表
        keys = list(self.simulator.agent_status.keys())

        # 获取从 constants.NUM_PREDATORS 开始往后 constants.NUM_PREYS 个键
        selected_keys = keys[constants.NUM_PREDATORS:constants.NUM_PREDATORS + constants.NUM_PREY]

        # 构建新的字典

        # all_actions =[]
        # all_pred_actions, all_prey_actions = actions[:constants.NUM_PREDATORS],actions[constants.NUM_PREDATORS:]
        # 独立处理每个组的动作
        for group_name in self.group_map.keys():
            if group_name == "predators":
                group_actions = actions[:constants.NUM_PREDATORS]
                agent_status = {k: self.simulator.agent_status[k] for k in list(self.simulator.agent_status.keys())[:constants.NUM_PREDATORS]}
            if group_name == "preys":
                group_actions = actions[constants.NUM_PREDATORS:]
                agent_status = {k: self.simulator.agent_status[k] for k in selected_keys}
            
            # 获取每个组的数据，并将其展开添加到主列表中
            group_states, group_rewards, group_dones, group_infos = self._step_group(group_name, group_actions,agent_status)
            new_state.extend(group_states)
            rewards.extend(group_rewards)
            dones.extend(group_dones)
            infos.extend(group_infos)
        infos = {
            f"info_{i}": info_item for i, info_item in enumerate(infos)
        }
        terminated = all(dones)   
        # 将 observations 列表转换为 numpy 数组
        new_observations = np.array(new_state, dtype=np.float32)
        # 调用模拟器的其他方法
        self.simulator.add_food()  # 传递时间间隔
        self.simulator.prey_hunt()
        self.simulator.check_collisions()
        self.simulator.decrease_health()  # 更新健康值
        self.simulator.remove_dead()  # 清理死亡个体

        return new_observations, rewards, terminated, truncated, infos


    # def _step_group(self, group_name, group_actions):
    #     # 执行每个组的动作，并获取新的状态、奖励、是否完成和信息
    #     new_observations = []
    #     rewards = []
    #     dones = []
    #     infos = []
    #     # print(np.shape(group_actions))
    #     group = getattr(self.simulator, group_name)
    #     for agent, action in zip(group, group_actions):
    #         agent.move_strategy(action)
    #         agent.move(constants.CONTROL_PANEL_WIDTH, self.simulator.screen_width, self.simulator.screen_height, self.simulator.obstacles)
            
    #         new_observations.append(agent.get_observe_info())
    #         rewards.append(self._compute_reward(agent, group_name))
    #         dones.append(not agent.is_alive)  # 这里假设死亡标志环境结束
    #         infos.append({})  # 可以添加更多的调试信息

    #     return new_observations, rewards, dones, infos
    # def _step_group(self, group_name, group_actions):
    #     # 执行每个组的动作，并获取新的状态、奖励、是否完成和信息
    #     new_observations = []
    #     rewards = []
    #     dones = []
    #     infos = []
        
    #     group = getattr(self.simulator, group_name)
    #     for agent, action in zip(group, group_actions):
    #         agent.move_strategy(action)
    #         agent.move(constants.CONTROL_PANEL_WIDTH, self.simulator.screen_width, self.simulator.screen_height, self.simulator.obstacles)
            
    #         new_observations.append(agent.get_observe_info())
    #         rewards.append(self._compute_reward(agent, group_name))
    #         dones.append(not agent.is_alive)  # 这里假设死亡标志环境结束
    #         infos.append({})  # 可以添加更多的调试信息

    #     return new_observations, rewards, dones, infos
    def _step_group(self, group_name, group_actions, agent_status):
        # 执行每个组的动作，并获取新的状态、奖励、是否完成和信息
        temp_observations = {}
        temp_rewards = {}
        temp_dones = {}
        temp_infos = {}

        group = getattr(self.simulator, group_name)
        
        for agent, action in zip(group, group_actions):
            agent.move_strategy(action)
            agent.move(constants.CONTROL_PANEL_WIDTH, self.simulator.screen_width, self.simulator.screen_height, self.simulator.obstacles)
            
            temp_observations[agent.name] = agent.get_observe_info()
            temp_rewards[agent.name] = self._compute_reward(agent, group_name)
            temp_dones[agent.name] = not agent.is_alive  # 这里假设死亡标志环境结束
            temp_infos[agent.name] = {}  # 可以添加更多的调试信息

        # 根据 agent_status 比对结果，生成最终的列表
        new_observations = []
        rewards = []
        dones = []
        infos = []
        
        for agent_name in agent_status.keys():
            if agent_name in temp_observations:
                new_observations.append(temp_observations[agent_name])
                rewards.append(temp_rewards[agent_name])
                dones.append(temp_dones[agent_name])
                infos.append(temp_infos[agent_name])
            else:
                new_observations.append(np.array(self.zero_list))  # 如果 agent 不存在，则返回0
                rewards.append(0)  # 如果 agent 不存在，则返回0
                dones.append(True)  # 如果 agent 不存在，则认为它完成了
                infos.append({})  # 如果 agent 不存在，则返回空信息字典

        return new_observations, rewards, dones, infos



    def _compute_reward(self, agent, group_name):
        # 根据组别计算奖励
        if group_name == 'predators':
            # 捕食者奖励
            return agent.health if agent.health > 0 else -1.0
        elif group_name == 'preys':
            # 猎物奖励
            return agent.health if agent.health > 0 else -1.0
        return 0

    def render(self, mode='human'):
        # Initialize Pygame screen if not already initialized
        if not hasattr(self, 'screen'):
            pygame.init()
            if mode == 'human':
                self.screen = pygame.display.set_mode((self.simulator.screen_width, self.simulator.screen_height))
            elif mode == 'rgb_array':
                self.screen = pygame.Surface((self.simulator.screen_width, self.simulator.screen_height))

        # Fill the background with black color
        self.screen.fill((0, 0, 0))
        # print(self.simulator.predators)
        # Draw models onto the screen
        self.simulator.draw_models(self.screen)

        # Update the display if mode is 'human'
        if mode == 'human':
            pygame.display.flip()
        elif mode == 'rgb_array':
            return self._get_rgb_array()

    def _get_rgb_array(self):
        # Convert Pygame surface to an RGB array (numpy)
        return np.transpose(
            np.array(pygame.surfarray.pixels3d(self.screen)),
            axes=(1, 0, 2)
        )

    def close(self):
        # 关闭环境
        pass




def generate_random_actions(num_agents, action_space):
    actions = []
    for _ in range(num_agents):
        action = action_space.sample()  # 从动作空间中采样一个随机动作
        # print(action)
        actions.append(action)
    return actions









def assign_algorithms_to_agents(len_agents, algorithm_names):
    """
    分配算法给每个智能体。

    参数:
    - agents: 智能体列表。
    - algorithm_names: 预定义的算法名称列表。

    返回:
    - 包含算法名称的列表，长度与agents列表相同。如果算法名称不足，则用'random'补充。
    """
    assigned_algorithms = []
    for i in range(len_agents):
        if i < len(algorithm_names):
            assigned_algorithms.append(algorithm_names[i])
        else:
            assigned_algorithms.append('random')
    return assigned_algorithms
def apply_algorithms_to_agents(agents, algorithms):
    """
    将算法分配给每个智能体。

    参数:
    - agents: 智能体列表。
    - algorithms: 已分配的算法名称列表。
    """
    for agent, algorithm in zip(agents, algorithms):
        agent.algorithm = algorithm  # 将算法分配给智能体

def run_random_simulation(env):
    
    # env = PredatorPreyEnv()
    observations,infos = env.reset()
    obs = observations
    # print("Returned Observation:", obs)
    # print("Observation Space Low:", env.observation_space.low)
    # print("Observation Space High:", env.observation_space.high)
    # print("Observation dtype:", obs.dtype)
    # print("Expected dtype:", env.observation_space.dtype)
    # assert env.observation_space.contains(obs), "Observation is out of bounds!"

    # print(np.shape(observations),end="---")
    # print(np.shape(env.observation_space))
    # check_env_specs(env)

    # check_env(env)
    # rollout = env.rollout(10)
    # print(f"rollout of {10} steps:", rollout)
    # print("Shape of the rollout TensorDict:", rollout.batch_size)
    

    # observations = env.reset()
    #print(np.shape(observation))
    done = False
    iteration = 0
    while not done:
        # actions = {
        #     'predators': generate_random_actions(len(env.simulator.predators), env.action_space),
        #     'preys': generate_random_actions(len(env.simulator.preys), env.action_space),
        # }
        actions = env.action_space.sample()  # 从动作空间中采样一个随机动作 # you can change this with your algorithm
        new_state, rewards, done, truncated,infos = env.step(actions)

        # 判断是否所有智能体都已经完成
        # done = all(all(done_group) for done_group in dones.values())
        iteration +=1

        # 渲染环境（可选）
        # env.render()
        if iteration % 100 == 1:   
            pass
            # print(f"iteration: {iteration}, num_predators: {len(env.simulator.predators)}, num_preys: {len(env.simulator.preys)}")

            # print(iteration,end="\t")
            # print(len(env.simulator.predators),end="\t")
            # print(len(env.simulator.preys))
        # 打印当前状态、奖励、是否结束
            # print(f"New State: {new_state}")
            # print(f"Rewards: {new_state}")
            # print(len(new_state))
            # print(f"Dones: {np.shape(done)}")
            # print(f"Dones length:{len(done)}")
            # print(f"Infos: {infos}")


if __name__ == "__main__":

    register(
        id='LISPredatorPreyEnv-v0',
        entry_point='gym_env:LISPredatorPreyEnv',
    )

    env = gym.make('LISPredatorPreyEnv-v0')
    obs,infos = env.reset()  # 重置环境并获取初始观测
    # print("Initial observation:", obs)
    # print(env.simulator.agent_status)

    for _ in range(10):  # 运行10个时间步
        action = env.action_space.sample()  # 随机采样一个动作
        ew_observations, rewards, terminated, truncated, infos = env.step(action)  # 采取一步行动
        # print(f"Observation: {obs}, Reward: {rewards}, Done: {terminated}, Info: {infos}")
        print(f"Observation: {type(obs)}, Reward: {np.shape(rewards)}, Done: {np.shape(terminated)}, Info: {np.shape(infos)}")

        # print(np.shape(obs))
        if terminated:
            obs = env.reset()  # 如果环境结束了，则重置环境
    
    check_env(env.unwrapped)
    check_env(env)

    run_random_simulation(env)




# File: /home/qrbao/Downloads/code/code/coderesearch12/tools/code2txt.py

import os

def collect_py_files_to_txt(parent_dir, output_file):
    with open(output_file, 'w') as outfile:
        for root, dirs, files in os.walk(parent_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    # print(file_path)
                    with open(file_path, 'r') as infile:
                        outfile.write(f'# File: {file_path}\n')
                        outfile.write(infile.read())
                        outfile.write('\n\n')

if __name__ == "__main__":
    parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))  # 获取上一层文件夹路径
    output_file = os.path.join(os.getcwd(), 'all_python_files3.txt')  # 输出文件路径

    collect_py_files_to_txt(parent_dir, output_file)
    print(f'所有的 .py 文件已整合到 {output_file}')



# File: /home/qrbao/Downloads/code/code/coderesearch12/tools/code3txt.py


import os

def combine_python_files_to_txt():
    # 获取当前脚本文件夹的上一级文件夹路径
    parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
    
    # 输出文件路径
    output_file = os.path.join(os.path.dirname(__file__), "combined_python_files.txt")
    
    # 打开输出文件（如果文件已存在，则覆盖；如果不存在，则新建）
    with open(output_file, 'w', encoding='utf-8') as outfile:
        # 遍历上一级文件夹中的所有文件
        for filename in os.listdir(parent_dir):
            # 构建文件路径
            file_path = os.path.join(parent_dir, filename)
            # 检查文件是否为 .py 文件并且是否是文件而不是目录
            if filename.endswith('.py') and os.path.isfile(file_path):
                with open(file_path, 'r', encoding='utf-8') as infile:
                    # 写入文件名作为注释
                    outfile.write(f"# {filename}\n")
                    # 读取 .py 文件内容并写入到输出文件中
                    outfile.write(infile.read())
                    # 每个文件内容之间添加两个换行符
                    outfile.write("\n\n")
    
    print(f"All .py files from {parent_dir} have been combined into {output_file}")

if __name__ == "__main__":
    combine_python_files_to_txt()


# File: /home/qrbao/Downloads/code/code/coderesearch12/tools/code4txt.py

import os

# 定义文件夹路径
parent_folder = os.path.dirname(os.path.abspath(__file__))  # 当前父文件夹
env_folder = os.path.join(parent_folder, 'env')  # env 文件夹路径

# 定义要生成的文本文件路径
output_file = os.path.join(parent_folder, 'combined_python_files.txt')

# 函数用于读取文件内容并写入到目标文件中
def append_file_content(file_path, output):
    with open(file_path, 'r') as f:
        content = f.read()
    with open(output, 'a') as f:
        f.write(f'\n\n# File: {file_path}\n\n')
        f.write(content)

# 获取父文件夹中的所有Python文件，并将内容写入目标文件
for root, _, files in os.walk(parent_folder):
    if root == env_folder:  # 跳过env文件夹
        continue
    for file in files:
        if file.endswith('.py'):
            append_file_content(os.path.join(root, file), output_file)

# 获取env文件夹中的所有Python文件，并将内容写入目标文件
for root, _, files in os.walk(env_folder):
    for file in files:
        if file.endswith('.py'):
            append_file_content(os.path.join(root, file), output_file)

print(f"所有Python文件的内容已成功粘贴到 {output_file} 中。")


# File: /home/qrbao/Downloads/code/code/coderesearch12/tools/narry_precess.py

# import numpy as np

# def process_matrix(matrix, target_shape=(5, 3)):
#     """
#     裁剪或填充矩阵，使其保持 target_shape 的大小。

#     参数:
#     - matrix: 输入的矩阵，形状为 (N, 3)
#     - target_shape: 目标形状，默认为 (5, 3)

#     返回:
#     - 处理后的矩阵，形状为 target_shape
#     """
#     current_shape = matrix.shape

#     # 确保输入矩阵的列数正确
#     assert current_shape[1] == target_shape[1], f"输入矩阵的列数应为 {target_shape[1]}，但得到了 {current_shape[1]}"

#     # 如果当前矩阵行数大于目标行数，则进行裁剪
#     if current_shape[0] > target_shape[0]:
#         processed_matrix = matrix[:target_shape[0], :]
#     # 如果当前矩阵行数小于目标行数，则进行填充
#     elif current_shape[0] < target_shape[0]:
#         padding_rows = target_shape[0] - current_shape[0]
#         padding = np.zeros((padding_rows, target_shape[1]))
#         processed_matrix = np.vstack((matrix, padding))
#     else:
#         processed_matrix = matrix

#     return processed_matrix

# # 示例使用
# input_matrix = np.array([
#     [1, 2, 3],
#     [4, 5, 6],
#     [7, 8, 9],
#     [10, 11, 12],
#     [13, 14, 15],
#     [16, 17, 18]
# ])

# processed_matrix = process_matrix(input_matrix)
# print(processed_matrix)


def process_matrix(matrix, target_shape=(5, 3)):
    """
    裁剪或填充矩阵，使其保持 target_shape 的大小。

    参数:
    - matrix: 输入的矩阵，形状为 (N, 3)
    - target_shape: 目标形状，默认为 (5, 3)

    返回:
    - 处理后的矩阵，形状为 target_shape
    """
    if not matrix:
        # 如果矩阵为空，直接填充目标大小的零矩阵
        return [[0] * target_shape[1] for _ in range(target_shape[0])]
    current_rows = len(matrix)
    current_cols = len(matrix[0]) if current_rows > 0 else 0

    # 确保输入矩阵的列数正确
    assert current_cols == target_shape[1], f"输入矩阵的列数应为 {target_shape[1]}，但得到了 {current_cols}"

    # 如果当前矩阵行数大于目标行数，则进行裁剪
    if current_rows > target_shape[0]:
        processed_matrix = matrix[:target_shape[0]]
    # 如果当前矩阵行数小于目标行数，则进行填充
    elif current_rows < target_shape[0]:
        processed_matrix = matrix[:]
        padding_rows = target_shape[0] - current_rows
        padding = [[0] * target_shape[1]] * padding_rows
        processed_matrix.extend(padding)
    else:
        processed_matrix = matrix

    return processed_matrix

matrix = []
print(process_matrix(matrix))


# File: /home/qrbao/Downloads/code/code/coderesearch12/tools/test.py

import gym
from gym import spaces
import numpy as np

class CustomEnv(gym.Env):
    def __init__(self):
        super(CustomEnv, self).__init__()

        # 定义动作空间和观察空间
        self.action_space = spaces.Discrete(2)  # 0表示向左，1表示向右
        self.observation_space = spaces.Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)

        # 初始化小车位置
        self.position = 0

    def reset(self):
        # 重置环境，将小车放置在起始位置
        self.position = 0
        return np.array([self.position])

    def step(self, action):
        # 执行动作，更新小车位置并返回奖励和观察结果
        if action == 0:
            self.position -= 1
        else:
            self.position += 1

        # 计算奖励
        reward = 1 if action == 1 else -1

        # 规定位置范围在 [0, 100] 之间
        self.position = np.clip(self.position, 0, 100)

        # 返回观察结果、奖励、是否终止和其他信息
        return np.array([self.position]), reward, False, {
   }

# 创建环境实例
env = CustomEnv()

# 测试环境
for episode in range(5):
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = env.action_space.sample()  # 随机选择动作
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
    print(f"Episode {episode + 1}, Total Reward: {total_reward}")
    