{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"abcdefg\"\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PPO for agent1 of type predator\n",
      "PPO result for agent1\n",
      "Running DQN for agent1 of type predator\n",
      "DQN result for agent1\n"
     ]
    }
   ],
   "source": [
    "def ppo_algorithm(agent_name, agent_type):\n",
    "    # Implement the PPO algorithm logic here\n",
    "    print(f\"Running PPO for {agent_name} of type {agent_type}\")\n",
    "    # Return a result or perform an action\n",
    "    return f\"PPO result for {agent_name}\"\n",
    "\n",
    "def dqn_algorithm(agent_name, agent_type):\n",
    "    # Implement the DQN algorithm logic here\n",
    "    print(f\"Running DQN for {agent_name} of type {agent_type}\")\n",
    "    # Return a result or perform an action\n",
    "    return f\"DQN result for {agent_name}\"\n",
    "\n",
    "# Store the algorithms in the dictionary\n",
    "pred_algo_predict = {\n",
    "    \"PPO\": ppo_algorithm,\n",
    "    \"DQN\": dqn_algorithm\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "agent_name = \"agent1\"\n",
    "agent_type = \"predator\"\n",
    "\n",
    "# Call the PPO algorithm\n",
    "result = pred_algo_predict[\"PPO\"](agent_name, agent_type)\n",
    "print(result)\n",
    "\n",
    "# Call the DQN algorithm\n",
    "result = pred_algo_predict[\"DQN\"](agent_name, agent_type)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pettingzoo.butterfly (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pettingzoo.butterfly\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo.butterfly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/qrbao/Downloads/code/code/coderesearch12/env/test.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch12/env/test.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpettingzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest\u001b[39;00m \u001b[39mimport\u001b[39;00m api_test\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch12/env/test.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpettingzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbutterfly\u001b[39;00m \u001b[39mimport\u001b[39;00m pistonball_v6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch12/env/test.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m pistonball_v6\u001b[39m.\u001b[39menv()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch12/env/test.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m api_test(env, num_cycles\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, verbose_progress\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/pettingzoo/butterfly/__init__.py:5\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(env_name):\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mreturn\u001b[39;00m deprecated_handler(env_name, __path__, \u001b[39m__name__\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/pettingzoo/utils/deprecated_module.py:52\u001b[0m, in \u001b[0;36mdeprecated_handler\u001b[0;34m(env_name, module_path, module_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m     50\u001b[0m \u001b[39m# This executes the module and will raise any exceptions\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# that would typically be raised by just `import blah`\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m spec\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mexec_module(module)\n\u001b[1;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/pettingzoo/butterfly/pistonball_v6.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpistonball\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpistonball\u001b[39;00m \u001b[39mimport\u001b[39;00m ManualPolicy, env, parallel_env, raw_env\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/pettingzoo/butterfly/pistonball/pistonball.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpymunk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpymunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpygame_util\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m EzPickle, seeding\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymunk'"
     ]
    }
   ],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.env()\n",
    "api_test(env, num_cycles=1000, verbose_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_1': tensor([1., 2., 3.]), 'agent_2': tensor([4., 5., 6.])}\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "import torch\n",
    "\n",
    "# Suppose this is the output from your PettingZoo environment's step function\n",
    "observations = {\n",
    "    \"agent_1\": [1.0, 2.0, 3.0],\n",
    "    \"agent_2\": [4.0, 5.0, 6.0],\n",
    "}\n",
    "\n",
    "# Convert dictionary values to tensors\n",
    "obs_tensors = {k: torch.tensor(v, dtype=torch.float32) for k, v in observations.items()}\n",
    "\n",
    "# Create a TensorDict from the dictionary\n",
    "obs_tensordict = TensorDict(obs_tensors, batch_size=[3])\n",
    "print(obs_tensors)\n",
    "\n",
    "# Now obs_tensordict contains your data in a TensorDict format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([[1, 2], [3, 4]])\n",
    "result = np.prod(arr, axis=0)\n",
    "print(result)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_list = [[0 for _ in range(3)] for _ in range(25)]\n",
    "print(zero_list)\n",
    "import numpy as np\n",
    "np.shape(zero_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(f\"{b}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for i in [123,2,2,2,2,1,23]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a  = np.array([1,2,3])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predator in []:\n",
    "    predator.color = (255, 255, 0)  # 高亮捕食者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from simulator import Simulator\n",
    "import constants\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_velocity(A = 100):\n",
    "    # Generate random angle\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    \n",
    "    # Generate random length less than A\n",
    "    length = np.random.uniform(0, A)\n",
    "    \n",
    "    # Calculate x and y based on angle and length\n",
    "    x = length * np.cos(angle)\n",
    "    y = length * np.sin(angle)\n",
    "    velocity = np.array([x, y], dtype=np.float32)\n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_range = max(600, 1000)\n",
    "num_entities = constants.NUM_PREDATORS + constants.NUM_PREY\n",
    "new_shape = (25 * num_entities, 3)\n",
    "obs_low = np.full(new_shape, -np.inf, dtype=np.float32)\n",
    "obs_high = np.full(new_shape, np.inf, dtype=np.float32)\n",
    "action_shape = (num_entities,2)\n",
    "action_speed_range = max(constants.PREY_MAX_SPEED,constants.PREY_MAX_SPEED)\n",
    "action_obs_low = np.full(action_shape, -1 * action_speed_range, dtype=np.float32)\n",
    "action_obs_high = np.full(action_shape, action_speed_range, dtype=np.float32)\n",
    "# obs_low = np.array([0, 0, 0] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "# obs_high = np.array([max_range, max_range, max_range] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "# self.observation_space_shape = (constants.NUM_PREDATORS+constants.NUM_PREY) * 3 * 25\n",
    "observation_space = spaces.Box(low=obs_low, high=obs_high,dtype=np.float32)\n",
    "action_space = spaces.Box(low=action_obs_low, high=action_obs_high, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (2250, 3), float32)\n",
      "(90, 2)\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "print(observation_space)\n",
    "print(action_shape)\n",
    "num_entities = constants.NUM_PREDATORS + constants.NUM_PREY\n",
    "print(num_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "No available video device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mLunarLander-v2\u001b[39m\u001b[39m\"\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset(seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m    action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()  \u001b[39m# this is where you would insert your policy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:215\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     logger\u001b[39m.\u001b[39mdeprecation(\n\u001b[1;32m    211\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[39m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    218\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:461\u001b[0m, in \u001b[0;36mLunarLander.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrawlist \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegs\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 461\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    462\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuous \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m], {}\n",
      "File \u001b[0;32m~/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:698\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    697\u001b[0m     pygame\u001b[39m.\u001b[39minit()\n\u001b[0;32m--> 698\u001b[0m     pygame\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mset_mode((VIEWPORT_W, VIEWPORT_H))\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31merror\u001b[0m: No available video device"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from simulator import Simulator\n",
    "import constants\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6750,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/LISPREY/lib/python3.9/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "max_range = 500\n",
    "obs_low = np.array([0, 0, 0] * 25 * (constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "obs_high = np.array([max_range, max_range, max_range] * 25 *(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "observation_space = spaces.Box(low=obs_low, high=obs_high,dtype=np.float32)\n",
    "print(np.shape(observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6750,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 3)\n"
     ]
    }
   ],
   "source": [
    "max_range = max(600, 800)\n",
    "num_entities = constants.NUM_PREDATORS + constants.NUM_PREY\n",
    "new_shape = (25 * num_entities, 3)\n",
    "obs_low = np.zeros(new_shape, dtype=np.float32)\n",
    "obs_high = np.full(new_shape, max_range, dtype=np.float32)\n",
    "\n",
    "# obs_low = np.array([0, 0, 0] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "# obs_high = np.array([max_range, max_range, max_range] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "# self.observation_space_shape = (constants.NUM_PREDATORS+constants.NUM_PREY) * 3 * 25\n",
    "observation_space = spaces.Box(low=obs_low, high=obs_high,dtype=np.float32)\n",
    "action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "print(np.shape(observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from simulator import Simulator\n",
    "import constants\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7] [8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "allalgorithms = [1,2,3,4,5,6,7,8,9,10,11]\n",
    "all_pred_algorithms, all_prey_algorithms = allalgorithms[:7],allalgorithms[7:]\n",
    "print(all_pred_algorithms,all_prey_algorithms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredatorPreyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(PredatorPreyEnv, self).__init__()\n",
    "        \n",
    "        # 初始化模拟器\n",
    "        self.simulator = Simulator(screen_width=3840, screen_height=2160)\n",
    "\n",
    "        self.group_map = {}\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 10_000\n",
    "        # 初始化观察和动作空间\n",
    "        max_range = max(600, 800)\n",
    "        num_entities = constants.NUM_PREDATORS + constants.NUM_PREY\n",
    "        new_shape = (25 * num_entities, 3)\n",
    "        obs_low = np.zeros(new_shape, dtype=np.float32)\n",
    "        obs_high = np.full(new_shape, max_range, dtype=np.float32)\n",
    "        # obs_low = np.array([0, 0, 0] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "        # obs_high = np.array([max_range, max_range, max_range] * 25*(constants.NUM_PREDATORS+constants.NUM_PREY))\n",
    "        # self.observation_space_shape = (constants.NUM_PREDATORS+constants.NUM_PREY) * 3 * 25\n",
    "        self.observation_space = spaces.Box(low=obs_low, high=obs_high,dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "        self.interation = 0\n",
    "        # 调用 reset 方法初始化环境\n",
    "        self.reset()\n",
    "\n",
    "    # def reset(self):\n",
    "    #     # 重置模拟器\n",
    "    #     all_pred_algorithms,all_prey_algorithms = self.reset_algorithm()\n",
    "    #     self.simulator.initialize(all_pred_algorithms,all_prey_algorithms)\n",
    "    #     self.map_agents_to_groups(self.simulator.predators,self.simulator.preys)\n",
    "    #     # 初始化环境信息（捕食者、猎物、食物和障碍物）\n",
    "    #     for predator in self.simulator.predators:\n",
    "    #         self._set_agent_env(predator)\n",
    "    #     for prey in self.simulator.preys:\n",
    "    #         self._set_agent_env(prey)\n",
    "    #     # 获取所有智能体的初始观测数据\n",
    "    #     observations = {}\n",
    "    #     for group_name in self.group_map.keys():\n",
    "    #         observations[group_name] = []\n",
    "    #         for agent in getattr(self.simulator, group_name):\n",
    "    #             observations[group_name].append(agent.get_observe_info())\n",
    "    #     info  = {}\n",
    "\n",
    "    #     # 返回包含所有智能体观测数据的字典\n",
    "    #     return observations,info\n",
    "        \n",
    "    def reset(self):\n",
    "        # 重置模拟器\n",
    "        all_pred_algorithms, all_prey_algorithms = self.reset_algorithm()\n",
    "        self.simulator.initialize(all_pred_algorithms, all_prey_algorithms)\n",
    "        self.map_agents_to_groups(self.simulator.predators, self.simulator.preys)\n",
    "        # 初始化环境信息（捕食者、猎物、食物和障碍物）\n",
    "        for predator in self.simulator.predators:\n",
    "            self._set_agent_env(predator)\n",
    "        for prey in self.simulator.preys:\n",
    "            self._set_agent_env(prey)\n",
    "        # 获取所有智能体的初始观测数据\n",
    "        all_observations = []\n",
    "        for group_name in self.group_map.keys():\n",
    "            for agent in getattr(self.simulator, group_name):\n",
    "                all_observations.extend(agent.get_observe_info())\n",
    "        \n",
    "        obs = np.array(all_observations, dtype=np.float32) #np.shape(all_observations) = (2250,3)\n",
    "        # print(np.shape(all_observations))\n",
    "        info = {}\n",
    "        # print(\"Initial Observation:\", obs)\n",
    "        # assert self.observation_space.contains(obs), \"Initial observation is out of bounds!\"\n",
    "\n",
    "        # 返回一个数组，符合 observation_space 的定义\n",
    "        return obs, info\n",
    "\n",
    "    def map_agents_to_groups(self,simPredators,simPreys):\n",
    "        self.group_map['predators'] = [predator.name for predator in simPredators]\n",
    "        self.group_map['preys'] = [prey.name for prey in simPreys]\n",
    "\n",
    "    def reset_algorithm(self):\n",
    "        prey_algorithms = [\"PPO\",\"PPO\",\"PPO\",\"DDPG\",\"DDPG\",\"DDPG\"]\n",
    "        pred_algorithms = [\"PPO\",\"PPO\",\"PPO\",\"DDPG\",\"DDPG\",\"DDPG\"]\n",
    "        all_pred_algorithms = assign_algorithms_to_agents(constants.NUM_PREDATORS,pred_algorithms)\n",
    "        all_prey_algorithms = assign_algorithms_to_agents(constants.NUM_PREY,prey_algorithms)\n",
    "        return all_pred_algorithms,all_prey_algorithms\n",
    "        # assigned_Predalgorithms = assign_algorithms_to_agents(self.simulator.predators, all_pred_algorithms)\n",
    "        # assigned_Preyalgorithms = assign_algorithms_to_agents(self.simulator.preys, all_prey_algorithms)\n",
    "\n",
    "    def _set_agent_env(self, agent):\n",
    "        agent.env_predators = self.simulator.predators\n",
    "        agent.env_prey = self.simulator.preys\n",
    "        agent.env_food = self.simulator.foods\n",
    "        agent.env_obstacles = self.simulator.obstacles\n",
    "\n",
    "    def step(self, actions):\n",
    "        new_state, rewards, dones, infos = {}, {}, {}, {}\n",
    "        self.current_step += 1\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        # self.simulator.check_events()\n",
    "\n",
    "        \n",
    "        # self.simulator.move_models()\n",
    "\n",
    "        # 独立处理每个组的动作\n",
    "        for group_name in self.group_map.keys():\n",
    "            group_actions = actions[group_name]\n",
    "            new_state[group_name], rewards[group_name], dones[group_name], infos[group_name] = self._step_group(group_name, group_actions)\n",
    "        self.simulator.add_food()  # 传递时间间隔\n",
    "        self.simulator.prey_hunt()\n",
    "        self.simulator.check_collisions()\n",
    "\n",
    "        # self.simulator.predator_hunt()\n",
    "        self.simulator.decrease_health()  # 更新健康值\n",
    "        self.simulator.remove_dead()  # 清理死亡个体\n",
    "        # self.simulator.draw_models(screen)\n",
    "\n",
    "        # sim.check_events()\n",
    "\n",
    "        \n",
    "        # sim.move_models()\n",
    "        # sim.add_food()  # 传递时间间隔\n",
    "        # sim.prey_hunt()\n",
    "        # sim.check_collisions()\n",
    "        # # sim.predator_hunt()\n",
    "        # # new_prey_born, new_predator_born = sim.applyGeneticAlgorithm()\n",
    "        # sim.decrease_health()  # 更新健康值\n",
    "        # sim.remove_dead()  # 清理死亡个体\n",
    "        # iteration_count += 1  # 增加迭代计数器\n",
    "        # sim.draw_models(screen)\n",
    "\n",
    "        # new_prey_born, new_predator_born = self.simulator.applyGeneticAlgorithm()\n",
    "\n",
    "        return new_state, rewards, dones, truncated, infos\n",
    "\n",
    "    def _step_group(self, group_name, group_actions):\n",
    "        # 执行每个组的动作，并获取新的状态、奖励、是否完成和信息\n",
    "        new_observations = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        infos = []\n",
    "        # print(np.shape(group_actions))\n",
    "        group = getattr(self.simulator, group_name)\n",
    "        for agent, action in zip(group, group_actions):\n",
    "            agent.move_strategy(action)\n",
    "            agent.move(constants.CONTROL_PANEL_WIDTH, self.simulator.screen_width, self.simulator.screen_height, self.simulator.obstacles)\n",
    "            \n",
    "            new_observations.append(agent.get_observe_info())\n",
    "            rewards.append(self._compute_reward(agent, group_name))\n",
    "            dones.append(not agent.is_alive)  # 这里假设死亡标志环境结束\n",
    "            infos.append({})  # 可以添加更多的调试信息\n",
    "\n",
    "        return new_observations, rewards, dones, infos\n",
    "\n",
    "    def _compute_reward(self, agent, group_name):\n",
    "        # 根据组别计算奖励\n",
    "        if group_name == 'predators':\n",
    "            # 捕食者奖励\n",
    "            return agent.health if agent.health > 0 else -1.0\n",
    "        elif group_name == 'preys':\n",
    "            # 猎物奖励\n",
    "            return agent.health if agent.health > 0 else -1.0\n",
    "        return 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Initialize Pygame screen if not already initialized\n",
    "        if not hasattr(self, 'screen'):\n",
    "            pygame.init()\n",
    "            if mode == 'human':\n",
    "                self.screen = pygame.display.set_mode((self.simulator.screen_width, self.simulator.screen_height))\n",
    "            elif mode == 'rgb_array':\n",
    "                self.screen = pygame.Surface((self.simulator.screen_width, self.simulator.screen_height))\n",
    "\n",
    "        # Fill the background with black color\n",
    "        self.screen.fill((0, 0, 0))\n",
    "\n",
    "        # Draw models onto the screen\n",
    "        self.simulator.draw_models(self.screen)\n",
    "\n",
    "        # Update the display if mode is 'human'\n",
    "        if mode == 'human':\n",
    "            pygame.display.flip()\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._get_rgb_array()\n",
    "\n",
    "    def _get_rgb_array(self):\n",
    "        # Convert Pygame surface to an RGB array (numpy)\n",
    "        return np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(self.screen)),\n",
    "            axes=(1, 0, 2)\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        # 关闭环境\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_actions(num_agents, action_space):\n",
    "    actions = []\n",
    "    for _ in range(num_agents):\n",
    "        action = action_space.sample()  # 从动作空间中采样一个随机动作\n",
    "        # print(action)\n",
    "        actions.append(action)\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def assign_algorithms_to_agents(len_agents, algorithm_names):\n",
    "    \"\"\"\n",
    "    分配算法给每个智能体。\n",
    "\n",
    "    参数:\n",
    "    - agents: 智能体列表。\n",
    "    - algorithm_names: 预定义的算法名称列表。\n",
    "\n",
    "    返回:\n",
    "    - 包含算法名称的列表，长度与agents列表相同。如果算法名称不足，则用'random'补充。\n",
    "    \"\"\"\n",
    "    assigned_algorithms = []\n",
    "    for i in range(len_agents):\n",
    "        if i < len(algorithm_names):\n",
    "            assigned_algorithms.append(algorithm_names[i])\n",
    "        else:\n",
    "            assigned_algorithms.append('random')\n",
    "    return assigned_algorithms\n",
    "def apply_algorithms_to_agents(agents, algorithms):\n",
    "    \"\"\"\n",
    "    将算法分配给每个智能体。\n",
    "\n",
    "    参数:\n",
    "    - agents: 智能体列表。\n",
    "    - algorithms: 已分配的算法名称列表。\n",
    "    \"\"\"\n",
    "    for agent, algorithm in zip(agents, algorithms):\n",
    "        agent.algorithm = algorithm  # 将算法分配给智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredPPO_1    PPO\n",
      "PredPPO_2    PPO\n",
      "PredPPO_3    PPO\n",
      "PredDDPG_4    DDPG\n",
      "PredDDPG_5    DDPG\n",
      "PredDDPG_6    DDPG\n",
      "Predrandom_7    random\n",
      "Predrandom_8    random\n",
      "Predrandom_9    random\n",
      "Predrandom_10    random\n",
      "Predrandom_11    random\n",
      "Predrandom_12    random\n",
      "Predrandom_13    random\n",
      "Predrandom_14    random\n",
      "Predrandom_15    random\n",
      "Predrandom_16    random\n",
      "Predrandom_17    random\n",
      "Predrandom_18    random\n",
      "Predrandom_19    random\n",
      "Predrandom_20    random\n",
      "Predrandom_21    random\n",
      "Predrandom_22    random\n",
      "Predrandom_23    random\n",
      "Predrandom_24    random\n",
      "Predrandom_24    random\n",
      "Predrandom_25    random\n",
      "Predrandom_26    random\n",
      "Predrandom_27    random\n",
      "Predrandom_28    random\n",
      "Predrandom_29    random\n",
      "Predrandom_30    random\n",
      "Predrandom_31    random\n",
      "Predrandom_32    random\n",
      "Predrandom_33    random\n",
      "Predrandom_34    random\n",
      "Predrandom_35    random\n",
      "Predrandom_36    random\n",
      "Predrandom_37    random\n",
      "Predrandom_38    random\n",
      "Predrandom_39    random\n",
      "Predrandom_40    random\n",
      "Predrandom_41    random\n",
      "Predrandom_42    random\n",
      "Predrandom_43    random\n",
      "Predrandom_44    random\n",
      "Predrandom_45    random\n",
      "Predrandom_46    random\n",
      "Predrandom_47    random\n",
      "Predrandom_48    random\n",
      "Predrandom_49    random\n",
      "Predrandom_50    random\n",
      "Predrandom_51    random\n",
      "Predrandom_52    random\n",
      "Predrandom_53    random\n",
      "Predrandom_54    random\n",
      "Predrandom_55    random\n",
      "Predrandom_56    random\n",
      "Predrandom_57    random\n",
      "Predrandom_58    random\n",
      "Predrandom_59    random\n",
      "Predrandom_60    random\n",
      "Predrandom_60    random\n",
      "Predrandom_61    random\n",
      "Predrandom_62    random\n",
      "Predrandom_63    random\n",
      "Predrandom_64    random\n",
      "Predrandom_65    random\n",
      "Predrandom_66    random\n",
      "Predrandom_67    random\n",
      "Predrandom_68    random\n",
      "Predrandom_69    random\n",
      "Predrandom_70    random\n",
      "Predrandom_71    random\n",
      "Predrandom_72    random\n",
      "Predrandom_73    random\n",
      "Predrandom_74    random\n",
      "Predrandom_75    random\n",
      "Predrandom_76    random\n",
      "Predrandom_77    random\n",
      "Predrandom_78    random\n",
      "Predrandom_79    random\n",
      "Predrandom_80    random\n",
      "PredPPO_81    PPO\n",
      "PredPPO_82    PPO\n",
      "PredPPO_83    PPO\n",
      "PredDDPG_84    DDPG\n",
      "PredDDPG_85    DDPG\n",
      "PredDDPG_86    DDPG\n",
      "Predrandom_87    random\n",
      "Predrandom_88    random\n",
      "Predrandom_89    random\n",
      "Predrandom_90    random\n",
      "Predrandom_91    random\n",
      "Predrandom_92    random\n",
      "Predrandom_93    random\n",
      "Predrandom_94    random\n",
      "Predrandom_95    random\n",
      "Predrandom_96    random\n",
      "Predrandom_97    random\n",
      "Predrandom_98    random\n",
      "Predrandom_99    random\n",
      "Predrandom_100    random\n",
      "Predrandom_101    random\n",
      "Predrandom_102    random\n",
      "Predrandom_103    random\n",
      "Predrandom_104    random\n",
      "Predrandom_105    random\n",
      "Predrandom_106    random\n",
      "Predrandom_107    random\n",
      "Predrandom_108    random\n",
      "Predrandom_108    random\n",
      "Predrandom_109    random\n",
      "Predrandom_110    random\n",
      "Predrandom_111    random\n",
      "Predrandom_112    random\n",
      "Predrandom_113    random\n",
      "Predrandom_114    random\n",
      "Predrandom_115    random\n",
      "Predrandom_116    random\n",
      "Predrandom_117    random\n",
      "Predrandom_117    random\n",
      "Predrandom_118    random\n",
      "Predrandom_119    random\n",
      "Predrandom_120    random\n",
      "Predrandom_121    random\n",
      "Predrandom_122    random\n",
      "Predrandom_123    random\n",
      "Predrandom_124    random\n",
      "Predrandom_125    random\n",
      "Predrandom_126    random\n",
      "Predrandom_127    random\n",
      "Predrandom_128    random\n",
      "Predrandom_129    random\n",
      "Predrandom_130    random\n",
      "Predrandom_131    random\n",
      "Predrandom_132    random\n",
      "Predrandom_133    random\n",
      "Predrandom_134    random\n",
      "Predrandom_135    random\n",
      "Predrandom_136    random\n",
      "Predrandom_137    random\n",
      "Predrandom_138    random\n",
      "Predrandom_139    random\n",
      "Predrandom_140    random\n",
      "Predrandom_141    random\n",
      "Predrandom_142    random\n",
      "Predrandom_143    random\n",
      "Predrandom_144    random\n",
      "Predrandom_145    random\n",
      "Predrandom_146    random\n",
      "Predrandom_147    random\n",
      "Predrandom_148    random\n",
      "Predrandom_149    random\n",
      "Predrandom_150    random\n",
      "Predrandom_150    random\n",
      "Predrandom_151    random\n",
      "Predrandom_152    random\n",
      "Predrandom_153    random\n",
      "Predrandom_154    random\n",
      "Predrandom_155    random\n",
      "Predrandom_156    random\n",
      "Predrandom_157    random\n",
      "Predrandom_158    random\n",
      "Predrandom_159    random\n",
      "Predrandom_160    random\n",
      "(2250, 3)---(2250, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "env = PredatorPreyEnv()\n",
    "observations,infos = env.reset()\n",
    "print(np.shape(observations),end=\"---\")\n",
    "print(np.shape(env.observation_space))\n",
    "# check_env_specs(env)\n",
    "# check_env(env)\n",
    "\n",
    "\n",
    "# rollout = env.rollout(10)\n",
    "# print(f\"rollout of {10} steps:\", rollout)\n",
    "# print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredPPO_161    PPO\n",
      "PredPPO_162    PPO\n",
      "PredPPO_163    PPO\n",
      "PredDDPG_164    DDPG\n",
      "PredDDPG_165    DDPG\n",
      "PredDDPG_166    DDPG\n",
      "Predrandom_167    random\n",
      "Predrandom_168    random\n",
      "Predrandom_169    random\n",
      "Predrandom_170    random\n",
      "Predrandom_171    random\n",
      "Predrandom_172    random\n",
      "Predrandom_173    random\n",
      "Predrandom_174    random\n",
      "Predrandom_175    random\n",
      "Predrandom_176    random\n",
      "Predrandom_177    random\n",
      "Predrandom_178    random\n",
      "Predrandom_179    random\n",
      "Predrandom_180    random\n",
      "Predrandom_181    random\n",
      "Predrandom_182    random\n",
      "Predrandom_183    random\n",
      "Predrandom_184    random\n",
      "Predrandom_185    random\n",
      "Predrandom_186    random\n",
      "Predrandom_187    random\n",
      "Predrandom_188    random\n",
      "Predrandom_189    random\n",
      "Predrandom_190    random\n",
      "Predrandom_191    random\n",
      "Predrandom_192    random\n",
      "Predrandom_193    random\n",
      "Predrandom_194    random\n",
      "Predrandom_195    random\n",
      "Predrandom_196    random\n",
      "Predrandom_197    random\n",
      "Predrandom_198    random\n",
      "Predrandom_199    random\n",
      "Predrandom_200    random\n",
      "Predrandom_201    random\n",
      "Predrandom_202    random\n",
      "Predrandom_203    random\n",
      "Predrandom_204    random\n",
      "Predrandom_205    random\n",
      "Predrandom_206    random\n",
      "Predrandom_207    random\n",
      "Predrandom_208    random\n",
      "Predrandom_209    random\n",
      "Predrandom_210    random\n",
      "Predrandom_211    random\n",
      "Predrandom_212    random\n",
      "Predrandom_213    random\n",
      "Predrandom_214    random\n",
      "Predrandom_215    random\n",
      "Predrandom_216    random\n",
      "Predrandom_217    random\n",
      "Predrandom_218    random\n",
      "Predrandom_219    random\n",
      "Predrandom_220    random\n",
      "Predrandom_221    random\n",
      "Predrandom_222    random\n",
      "Predrandom_223    random\n",
      "Predrandom_224    random\n",
      "Predrandom_225    random\n",
      "Predrandom_226    random\n",
      "Predrandom_227    random\n",
      "Predrandom_228    random\n",
      "Predrandom_228    random\n",
      "Predrandom_229    random\n",
      "Predrandom_230    random\n",
      "Predrandom_231    random\n",
      "Predrandom_232    random\n",
      "Predrandom_233    random\n",
      "Predrandom_234    random\n",
      "Predrandom_235    random\n",
      "Predrandom_236    random\n",
      "Predrandom_237    random\n",
      "Predrandom_238    random\n",
      "Predrandom_239    random\n",
      "Predrandom_240    random\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     actions \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mpredators\u001b[39m\u001b[39m'\u001b[39m: generate_random_actions(\u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39mpredators), env\u001b[39m.\u001b[39maction_space),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mpreys\u001b[39m\u001b[39m'\u001b[39m: generate_random_actions(\u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39msimulator\u001b[39m.\u001b[39mpreys), env\u001b[39m.\u001b[39maction_space),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     new_state, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# 判断是否所有智能体都已经完成\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Downloads/code/code/coderesearch6/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(\u001b[39mall\u001b[39m(done_group) \u001b[39mfor\u001b[39;00m done_group \u001b[39min\u001b[39;00m dones\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "observations = env.reset()\n",
    "#print(np.shape(observation))\n",
    "done = False\n",
    "iteration = 0\n",
    "while not done:\n",
    "    actions = {\n",
    "        'predators': generate_random_actions(len(env.simulator.predators), env.action_space),\n",
    "        'preys': generate_random_actions(len(env.simulator.preys), env.action_space),\n",
    "    }\n",
    "\n",
    "    new_state, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "    # 判断是否所有智能体都已经完成\n",
    "    done = all(all(done_group) for done_group in dones.values())\n",
    "    iteration +=1\n",
    "\n",
    "    # 渲染环境（可选）\n",
    "    env.render()\n",
    "    if iteration == 10:   \n",
    "            \n",
    "        print(iteration)\n",
    "    # 打印当前状态、奖励、是否结束\n",
    "        # print(f\"New State: {new_state}\")\n",
    "        # print(f\"Rewards: {rewards}\")\n",
    "        print(f\"Dones: {np.shape(dones)}\")\n",
    "        print(f\"Dones length:{len(dones)}\")\n",
    "        # print(f\"Infos: {infos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class Simulator:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "\n",
    "    def increment(self):\n",
    "        self.value += 1\n",
    "        return self.value\n",
    "\n",
    "# 创建第一个实例\n",
    "env1 = Simulator()\n",
    "print(env1.increment())  # 输出: 1\n",
    "print(env1.increment())  # 输出: 2\n",
    "\n",
    "# 创建第二个实例\n",
    "env1 = Simulator()\n",
    "print(env1.increment())  # 输出: 1\n",
    "print(env1.increment())  # 输出: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    adversary: CompositeSpec(\n",
      "        observation: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([1, 2, 14]),\n",
      "            space=None,\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([1, 2])),\n",
      "    agent: CompositeSpec(\n",
      "        observation: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([1, 1, 12]),\n",
      "            space=None,\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([1, 1])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement, RandomSampler\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage, LazyMemmapStorage\n",
    "\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.envs import PettingZooEnv, ExplorationType, set_exploration_type\n",
    "\n",
    "from torchrl.modules import AdditiveGaussianWrapper, MultiAgentMLP, ProbabilisticActor, TanhDelta, TanhNormal\n",
    "\n",
    "from torchrl.objectives import DDPGLoss,ClipPPOLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        self.seed = 1337\n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        self.agent_keys = ['adversary', 'agent']\n",
    "        \n",
    "        self.num_agents = {'adversary':2, 'agent': 1}\n",
    "        \n",
    "        \n",
    "        self.frames_per_batch  = 1_000 \n",
    "        self.n_rollout_steps   = 5\n",
    "        \n",
    "        self.n_iters      = 10  \n",
    "        self.max_steps    = 1000 \n",
    "        self.max_grad_norm = 1.0  \n",
    "\n",
    "        self.total_frames = self.frames_per_batch * self.n_iters\n",
    "        \n",
    "        self.n_optimiser_steps = 100  \n",
    "        self.memory_size      = {'adversary': 1_000_000, 'agent': 1_000_000}\n",
    "        self.train_batch_size = {'adversary': 128,       'agent': 128}     \n",
    "        self.gamma            = {'adversary': 0.99,      'agent': 0.99}\n",
    "        self.polyak_tau       = {'adversary': 0.005,     'agent': 0.005}\n",
    "        self.lr_actor         = {'adversary': 3e-4,      'agent': 3e-4}\n",
    "        self.lr_critic        = {'adversary': 3e-4,      'agent':3e-4}\n",
    "        \n",
    "        self.replay_buffers       = {}\n",
    "        self.exploration_policies = {}\n",
    "        self.target_updaters      = {}\n",
    "        self.losses               = {}\n",
    "        self.optimizers           = {}\n",
    "        \n",
    "        self.use_vmas = True\n",
    "        self.num_obstacles = 2\n",
    "    def env(self):\n",
    "\n",
    "        base_env = VmasEnv(\n",
    "            scenario=\"simple_tag\",\n",
    "            num_envs=self.frames_per_batch // self.max_steps,\n",
    "            device=self.device,\n",
    "            seed=self.seed,\n",
    "            continuous_actions=True,\n",
    "            num_good_agents=self.num_agents['agent'],\n",
    "            num_adversaries=self.num_agents['adversary'],\n",
    "            num_landmarks=self.num_obstacles,\n",
    "            max_steps=self.max_steps,\n",
    "        )\n",
    "\n",
    "        # print(\"group_map:\",        base_env.group_map)\n",
    "        # print(\"action_spec:\",      base_env.full_action_spec)\n",
    "        # print(\"reward_spec:\",      base_env.full_reward_spec)\n",
    "        # print(\"done_spec:\",        base_env.full_done_spec)\n",
    "        print(\"observation_spec:\", base_env.observation_spec)\n",
    "        # print(\"action_keys:\",      base_env.action_keys)\n",
    "        # print(\"reward_keys:\",      base_env.reward_keys)\n",
    "        # print(\"done_keys:\",        base_env.done_keys)\n",
    "\n",
    "a = MultiAgentTrainer()\n",
    "a.env()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LISPREY)",
   "language": "python",
   "name": "lisprey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
